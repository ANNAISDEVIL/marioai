{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training an agent to play Super Mario using player recorderd data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will learn how to use player generated data to train a neural network to play Super Mario. The results will be evaluated against the results from the Q_Learner exercise. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating data\n",
    "First, you will have to generate some data for the neural network to train with.\n",
    "You will have the most fun playing with a USB-Controller but if you have none, you can set the following variable to false to use the keyboard:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "USE_GAMEPAD = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gamepad Controlls Xbox(Playstation):<br>\n",
    "A(X): Jump<br>\n",
    "B(O): Run<br>\n",
    "Dpad Right: Move Right<br>\n",
    "Dpad Left: Move Left<br>\n",
    "Dpad Down: Duck<br>\n",
    "\n",
    "Keyboard Controlls:<br>\n",
    "S: Jump<br>\n",
    "A: Run<br>\n",
    "Arrowkey Right: Move Right<br>\n",
    "Arrowkey Left: Move Left<br>\n",
    "Arrowkey Down: Duck<br>\n",
    "G: Deactivate Matrix Grid<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import subprocess\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from gym_setup import Env\n",
    "from gamepad_controller import GamepadController\n",
    "from keyboard_controller import KeyboardController\n",
    "import numpy as np\n",
    "\n",
    "level = os.path.join(\"levels\", \"RoughTerrainLevel.lvl\") # TODO: Use a very easy level for this exercise\n",
    "\n",
    "try:\n",
    "    with subprocess.Popen(['java', '-jar', 'server.jar'], shell=True) as server:\n",
    "        env = Env(visible=True, port=8080, level=level, run_server=False).env\n",
    "        if USE_GAMEPAD:\n",
    "            controller = GamepadController(env)\n",
    "        else:\n",
    "            controller = KeyboardController(env)\n",
    "        while True:\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            action = controller.read()\n",
    "\n",
    "            observations = [observation]\n",
    "            actions = [action]\n",
    "            rewards = [0]  # No reward at first time step, because no action was taken yet\n",
    "            terminals = [done]\n",
    "\n",
    "            while not done:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                action = controller.read()\n",
    "\n",
    "                observations.append(observation)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                terminals.append(done)\n",
    "\n",
    "            dataset_path = os.path.join(\"data\", \"datasets\", os.path.split(level)[1] + \".h5\")\n",
    "            if os.path.isfile(dataset_path):\n",
    "                dataset = MDPDataset.load(dataset_path)\n",
    "                dataset.append(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                np.asarray(terminals))\n",
    "            else:\n",
    "                dataset = MDPDataset(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                        np.asarray(terminals), discrete_action=True)\n",
    "            dataset.dump(dataset_path)\n",
    "            stats = dataset.compute_stats()\n",
    "            mean = stats['return']['mean']\n",
    "            std = stats['return']['std']\n",
    "            print(f'mean: {mean}, std: {std}')\n",
    "except ConnectionResetError:\n",
    "    # Finish\n",
    "    pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Use the generated data to train a policy\n",
    "Now that you have generated some data for the neural network to train with, let's begin with the training.\n",
    "For the purpose of this exercise we will use the Offline RL Python library d3rlpy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Choosing an algorithm\n",
    "![DQN](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/dqn.PNG)\n",
    "\n",
    "#### Why we chose DQN? <br>\n",
    "Our main goal was to display how good data influences the effect of an RL algorithm.\n",
    "Creating measuring and learning data for a reinforcement Learning algorithm often consumes a lot of ressources,\n",
    "this leads to the conclusion that you can not afford to create a lot of learning data.\n",
    "Therefore in this exercise we try to teach an offline RL algorithm to work with a limited amount of data (1 hour of player\n",
    "created data per level) to solve complex mario levels. To compare our results we trained a qlearner for 10 000 episodes\n",
    "on the chosen levels. <br>\n",
    "Therefor we chose the DQN algorithm as our offline algorithm since it's Algorithm is based on a similiar groundwork as\n",
    "the qlearner. Additionally it has a high efficency. After recording player based data and qlearner data, we train a player\n",
    "and a qlearner model for the maps to compare their individual results with each other. <br>\n",
    "We expect our data to perform better than the qlearner data even tho it is significantly less data. This is because we\n",
    "as players only have to master the mechanics of the game and can plan ahead from the first episode we play, while the qlearner\n",
    "needs to learn everything from scratch. Since we evaluate mainly complex levels this leads the result, that the qlearner\n",
    "will need a lot of time to learn to finish the level or might not even be able to finish the level at all. Comparably\n",
    "we expect our data to have a 100% win ratio on the levels, which leads to better learning data for the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Setup the training\n",
    "Let's setup some parameters before the training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from data.datasets.getDatasets import getDataset\n",
    "from gym_setup import Env\n",
    "import d3rlpy\n",
    "import pathlib\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import socket\n",
    "from contextlib import closing\n",
    "\n",
    "MODEL_DIR = pathlib.Path(\"data\", \"models\")\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir(parents=True)\n",
    "\n",
    "# Environment settings\n",
    "port = 8081\n",
    "run_server = True\n",
    "visible = False\n",
    "\n",
    "# Training parameters\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0003\n",
    "target_update_interval = 3000\n",
    "n_epochs = 1000\n",
    "test_size = 0.1\n",
    "batch_size = 2\n",
    "n_frames = 1\n",
    "n_steps = 40\n",
    "use_gpu = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Training time!\n",
    "To start the training run the next cell. If you want to see the progress of your training you can adittionaly open a new terminal and run ``pipenv run board`` to see the Tensorboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    env = Env(visible=visible, port=8083, level=level, run_server=run_server).env\n",
    "\n",
    "    dataset = getDataset()\n",
    "\n",
    "    train_episodes, test_episodes = train_test_split(dataset, test_size=test_size)\n",
    "\n",
    "    dqn = d3rlpy.algos.DQN(learning_rate=learning_rate, gamma=gamma, use_gpu=use_gpu,\n",
    "                           target_update_interval=target_update_interval, batch_size=batch_size)\n",
    "\n",
    "    # train offline\n",
    "    dqn.build_with_dataset(dataset)\n",
    "    # set environment in scorer function\n",
    "    evaluate_scorer = evaluate_on_environment(env)\n",
    "    # evaluate algorithm on the environment\n",
    "    rewards = evaluate_scorer(dqn)\n",
    "    name = 'marioai_%s_%s_%s_%s_%s' % (level.split('/')[-1], gamma, learning_rate, target_update_interval, n_epochs)\n",
    "    currentMax = -100000\n",
    "    dqn_max = copy.deepcopy(dqn)\n",
    "\n",
    "    for epoch, metrics in (dqn.fitter(train_episodes, eval_episodes=test_episodes, tensorboard_dir='runs', experiment_name=name, n_epochs=n_epochs, scorers={'environment': evaluate_scorer})):\n",
    "        if metrics.get(\"environment\") > currentMax:\n",
    "            currentMax = metrics.get(\"environment\")\n",
    "            dqn_max.copy_q_function_from(dqn)\n",
    "        else:\n",
    "            dqn.copy_q_function_from(dqn_max)\n",
    "\n",
    "    model_file = pathlib.Path(MODEL_DIR, name + \".pt\")\n",
    "    dqn.save_model(model_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 See what worked\n",
    "Now let's see if the training did something:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = Env(visible=True, level=level, port=8082).env\n",
    "dqn = DQN()\n",
    "dqn.build_with_dataset(getDataset())\n",
    "dqn.load_model('data/models/model.pt')\n",
    "\n",
    "while True:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(dqn.predict([observation])[0])\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f'finished episode, total_reward: {total_reward}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Offline RL vs Online RL \n",
    "Now we want to compare the approach from exercise 1 where an online Q_Learner was used with the results one can get with the offline RL approach."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.0 Reproducibility\n",
    "To compare as fairly as possible we ran both the Online Q_Learner as well as the offline Deep Q Network until a plateau of performance has been reached."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Easy level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 232     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_q_learner_232.gif)    |\n",
    "| Deep Q Network  | 280    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_dqn_280.gif)    |\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Medium level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 176     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_q_learner_176.gif)    |\n",
    "| Deep Q Network  | 193    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_dqn_193.gif)    |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Hard level\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | -559     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_q_learner_-559.gif)    |\n",
    "| Deep Q Network  | -34    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_dqn_-34.gif)    |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Summary\n",
    "\n",
    "![Summary](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/level-summary.png)\n",
    "\n",
    "\n",
    "Both the data of our played episodes aswell as the data of the qlearner had the same reward settings. Our approach\n",
    "for the rewards was simple is best. The goal was to complete complex levels and see how good it performs with a small amount\n",
    "of good data compared to a (comparably) big amount of qlearning data (which itself had to learn the level first).\n",
    "With a very good but small dataset we now had to trained the network with each datasets for 100 epochs. Both had once again\n",
    "the same hyperparameters for the training to create a fair comparison. Our data performed much better tho. This is because\n",
    "we reached the goal from the beginning on and tried out different ways to do so, giving the Network the possibility to learn\n",
    "from different possible actions that lead to the goal. Meanwhile the qlearner itself had problems completing the levels in\n",
    "10 000 episodes since they were \"too\" complex to learn in such a short time without any reference data. This is made visible\n",
    "in all 3 grphics. At some point the qlearner completed the level but didn't optimize it's Qtable yet, which leads to our\n",
    "data resulting in having better Rewards in every level. The graph of the hard level has negative rewards for both our data\n",
    "model aswell as the qlearner data model, since both did not manage to complete the level. The level was too complex for\n",
    "the data since it needed alot of climbing and even tho the data for that level was good aswell the network didn't manage to\n",
    "learn to climb over a high wall."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Performance over 100 level\n",
    "\n",
    "// TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('marioai-iKFqD-69': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "4fefe5704b33e4500f5833ae6032bcef53ee790cf4ffa6448116d63501fea73e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}