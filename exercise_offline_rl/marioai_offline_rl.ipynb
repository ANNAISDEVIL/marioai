{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training an agent to play Super Mario using player recorderd data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will learn how to use player generated data to train a neural network to play Super Mario. The results will be evaluated against the results from the Q_Learner exercise. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating data\n",
    "First, you will have to generate some data for the neural network to train with.\n",
    "You will have the most fun playing with a USB-Controller but if you have none, you can set the following variable to false to use the keyboard:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "USE_GAMEPAD = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gamepad Controlls Xbox(Playstation):<br>\n",
    "A(X): Jump<br>\n",
    "B(O): Run<br>\n",
    "Dpad Right: Move Right<br>\n",
    "Dpad Left: Move Left<br>\n",
    "Dpad Down: Duck<br>\n",
    "\n",
    "Keyboard Controlls:<br>\n",
    "S: Jump<br>\n",
    "A: Run<br>\n",
    "Arrowkey Right: Move Right<br>\n",
    "Arrowkey Left: Move Left<br>\n",
    "Arrowkey Down: Duck<br>\n",
    "G: Deactivate Matrix Grid<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import subprocess\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from gym_setup import Env\n",
    "from gamepad_controller import GamepadController\n",
    "from keyboard_controller import KeyboardController\n",
    "import numpy as np\n",
    "\n",
    "level = os.path.join(\"levels\", \"RoughTerrainLevel.lvl\") # TODO: Use a very easy level for this exercise\n",
    "\n",
    "try:\n",
    "    with subprocess.Popen(['java', '-jar', 'server.jar'], shell=True) as server:\n",
    "        env = Env(visible=True, port=8080, level=level, run_server=False).env\n",
    "        if USE_GAMEPAD:\n",
    "            controller = GamepadController(env)\n",
    "        else:\n",
    "            controller = KeyboardController(env)\n",
    "        while True:\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            action = controller.read()\n",
    "\n",
    "            observations = [observation]\n",
    "            actions = [action]\n",
    "            rewards = [0]  # No reward at first time step, because no action was taken yet\n",
    "            terminals = [done]\n",
    "\n",
    "            while not done:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                action = controller.read()\n",
    "\n",
    "                observations.append(observation)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                terminals.append(done)\n",
    "\n",
    "            dataset_path = os.path.join(\"data\", \"datasets\", os.path.split(level)[1] + \".h5\")\n",
    "            if os.path.isfile(dataset_path):\n",
    "                dataset = MDPDataset.load(dataset_path)\n",
    "                dataset.append(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                np.asarray(terminals))\n",
    "            else:\n",
    "                dataset = MDPDataset(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                        np.asarray(terminals), discrete_action=True)\n",
    "            dataset.dump(dataset_path)\n",
    "            stats = dataset.compute_stats()\n",
    "            mean = stats['return']['mean']\n",
    "            std = stats['return']['std']\n",
    "            print(f'mean: {mean}, std: {std}')\n",
    "except ConnectionResetError:\n",
    "    # Finish\n",
    "    pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Use the generated data to train a policy\n",
    "Now that you have generated some data for the neural network to train with, let's begin with the training.\n",
    "For the purpose of this exercise we will use the Offline RL Python library d3rlpy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Choosing an algorithm\n",
    "![DQN](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/dqn.PNG)\n",
    "\n",
    "// TODO: Why we chose DQN <br>\n",
    "Our main goal was to display how good data influences the effect of an RL algorithm.\n",
    "Creating measuring and learning data for an reinforcement Learning algorithm needs a lot of ressources,\n",
    "this leads to the conclusion that you can not afford to create a lot of learning data.\n",
    "We trained our Deep Q-Network with one hour of data played by ourselfs for each map.\n",
    "Even with such a small amount of data our Mario AI learned to complete complex levels.\n",
    "To display the weight of that \"good\" data we compared it to a Q-learner which learned for 10000 episodes to train a\n",
    "a model for the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Setup the training\n",
    "Let's setup some parameters before the training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from data.datasets.getDatasets import getDataset\n",
    "from gym_setup import Env\n",
    "import d3rlpy\n",
    "import pathlib\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import socket\n",
    "from contextlib import closing\n",
    "\n",
    "MODEL_DIR = pathlib.Path(\"data\", \"models\")\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir(parents=True)\n",
    "\n",
    "# Environment settings\n",
    "port = 8081\n",
    "run_server = True\n",
    "visible = False\n",
    "\n",
    "# Training parameters\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0003\n",
    "target_update_interval = 3000\n",
    "n_epochs = 1000\n",
    "test_size = 0.1\n",
    "batch_size = 2\n",
    "n_frames = 1\n",
    "n_steps = 40\n",
    "use_gpu = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Training time!\n",
    "To start the training run the next cell. If you want to see the progress of your training you can adittionaly open a new terminal and run ``pipenv run board`` to see the Tensorboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    env = Env(visible=visible, port=8083, level=level, run_server=run_server).env\n",
    "\n",
    "    dataset = getDataset()\n",
    "\n",
    "    train_episodes, test_episodes = train_test_split(dataset, test_size=test_size)\n",
    "\n",
    "    dqn = d3rlpy.algos.DQN(learning_rate=learning_rate, gamma=gamma, use_gpu=use_gpu,\n",
    "                           target_update_interval=target_update_interval, batch_size=batch_size)\n",
    "\n",
    "    # train offline\n",
    "    dqn.build_with_dataset(dataset)\n",
    "    # set environment in scorer function\n",
    "    evaluate_scorer = evaluate_on_environment(env)\n",
    "    # evaluate algorithm on the environment\n",
    "    rewards = evaluate_scorer(dqn)\n",
    "    name = 'marioai_%s_%s_%s_%s_%s' % (level.split('/')[-1], gamma, learning_rate, target_update_interval, n_epochs)\n",
    "    currentMax = -100000\n",
    "    dqn_max = copy.deepcopy(dqn)\n",
    "\n",
    "    for epoch, metrics in (dqn.fitter(train_episodes, eval_episodes=test_episodes, tensorboard_dir='runs', experiment_name=name, n_epochs=n_epochs, scorers={'environment': evaluate_scorer})):\n",
    "        if metrics.get(\"environment\") > currentMax:\n",
    "            currentMax = metrics.get(\"environment\")\n",
    "            dqn_max.copy_q_function_from(dqn)\n",
    "        else:\n",
    "            dqn.copy_q_function_from(dqn_max)\n",
    "\n",
    "    model_file = pathlib.Path(MODEL_DIR, name + \".pt\")\n",
    "    dqn.save_model(model_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 See what worked\n",
    "Now let's see if the training did something:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = Env(visible=True, level=level, port=8082).env\n",
    "dqn = DQN()\n",
    "dqn.build_with_dataset(getDataset())\n",
    "dqn.load_model('data/models/model.pt')\n",
    "\n",
    "while True:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(dqn.predict([observation])[0])\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f'finished episode, total_reward: {total_reward}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Offline RL vs Online RL \n",
    "Now we want to compare the approach from exercise 1 where an online Q_Learner was used with the results one can get with the offline RL approach."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.0 Reproducibility\n",
    "To compare as fairly as possible we ran both the Online Q_Learner as well as the offline Deep Q Network until a plateau of performance has been reached."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Easy level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 232     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_q_learner_232.gif)    |\n",
    "| Deep Q Network  | 280    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_dqn_280.gif)    |\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Medium level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 176     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_q_learner_176.gif)    |\n",
    "| Deep Q Network  | 193    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_dqn_193.gif)    |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Hard level\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | -559     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_q_learner_-559.gif)    |\n",
    "| Deep Q Network  | -34    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_dqn_-34.gif)    |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Summary\n",
    "\n",
    "![Summary](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/level-summary.png)\n",
    "\n",
    "\n",
    "// TODO: Explain<br>\n",
    "To create a fair comparison we trained the DQN with one hour of recorded data of the level and let a Q_Learner train\n",
    "the same level for 10000 episodes. Even tho our played data has less episodes and steps our data has the advantage,\n",
    "that we as player are aware of the whole level from the beginning. This leading to the fact that we could plan ahead\n",
    "even in the first episode we played. We only had to learn the game mechanics, while the Q_learning algorithm had to\n",
    "learn everything from the beginning. This leads to the results, that our data had more winning episodes and more consistent data.\n",
    "On the other hand the Q_Learning algorithm could produce data much faster."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Performance over 100 level\r\n",
    "\r\n",
    "// TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('marioai-iKFqD-69': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "4fefe5704b33e4500f5833ae6032bcef53ee790cf4ffa6448116d63501fea73e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}