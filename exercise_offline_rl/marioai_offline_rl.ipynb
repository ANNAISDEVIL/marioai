{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training an agent to play Super Mario using player recorded data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will learn how to use player generated data to train a neural network to play Super Mario. The results will be evaluated against the results from the Q_Learner exercise. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating data\n",
    "First, you will have to generate some data for the neural network to train with.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Player generated data.\n",
    "\n",
    "![Mario](https://media1.giphy.com/media/aX0RqLt2ARSW4/giphy.gif?cid=ecf05e47fnkts3fqh25tj9v8noh9vnccwo4x0ey4zpdxc7ft&rid=giphy.gif&ct=g)\n",
    "\n",
    "\n",
    "To achieve the best possible results, the training algorithm needs the best possible data. In this case that means player generated data.\n",
    "\n",
    "\n",
    "You will have the most fun playing with a USB-Controller but if you have none, you can set the following variable to ``False`` to use the keyboard:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Don't forget to run me\n",
    "USE_GAMEPAD = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Controls\n",
    "|            \t| Keyboard    \t| Xbox       \t| Playstation \t|\n",
    "|------------\t|-------------\t|------------\t|-------------\t|\n",
    "| Jump       \t| S           \t| A          \t| X           \t|\n",
    "| Sprint     \t| A           \t| B          \t| O           \t|\n",
    "| Move Right \t| Arrow Right \t| Dpad Right \t| Dpad Right  \t|\n",
    "| Move Left  \t| Arrow Left  \t| Dpad Left  \t| Dpad Left   \t|\n",
    "| Duck       \t| Arrow Down  \t| Dpad Down  \t| Dpad Down   \t|"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Level\n",
    "The level we will be using for this exercise will be a very easy one to minimize the training time.\n",
    "\n",
    "However, if you would like to try different levels, we encourage you to do so by changing the ``level`` variable below to a different one from the ``exercise_offline_rl\\levels`` folder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup the imports and global variables. Run this cell again if you encounter any import errors.\n",
    "import copy\n",
    "import d3rlpy\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gym_setup import Env\n",
    "from Controller.gamepad_controller import GamepadController\n",
    "from Controller.keyboard_controller import KeyboardController\n",
    "from data.datasets.getDatasets import getDataset\n",
    "%load_ext tensorboard\n",
    "\n",
    "level_name = \"OneCliffLevel.lvl\"\n",
    "# ^ change here if you want to try a different level.\n",
    "\n",
    "level = os.path.join(\"levels\", level_name)\n",
    "\n",
    "dataset_path = os.path.join(\n",
    "    \"data\", \"datasets\", os.path.split(level)[1] + \".h5\")\n",
    "\n",
    "dataset_path_rand = os.path.join(\n",
    "    \"data\", \"datasets\", os.path.split(level)[1] + \".random.h5\")\n",
    "\n",
    "MODEL_DIR = pathlib.Path(\"data\", \"models\")\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir(parents=True)\n",
    "\n",
    "\n",
    "print(\"Done!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start the game run the next cell. If you think you have enough data just close the game window and move on to the next cell.\n",
    "\n",
    "Note: We have pregenerated some training data for your convenience which will be used in addition to your data to train the model.\n",
    "If you want to train with your own data only, go ahead and delete the data from ``exercise_offline_rl\\data\\datasets``."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's play!\n",
    "try:\n",
    "    env_play = Env(visible=True, level=level).env\n",
    "    if USE_GAMEPAD:\n",
    "        controller = GamepadController(env_play)\n",
    "    else:\n",
    "        controller = KeyboardController(env_play)\n",
    "    while True:\n",
    "        observation = env_play.reset()\n",
    "        done = False\n",
    "        action = controller.read()\n",
    "\n",
    "        observations = [observation]\n",
    "        actions = [action]\n",
    "        # No reward at first time step, because no action was taken yet\n",
    "        rewards = [0]\n",
    "        terminals = [done]\n",
    "\n",
    "        while not done:\n",
    "            observation, reward, done, info = env_play.step(action)\n",
    "            action = controller.read()\n",
    "\n",
    "            observations.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            terminals.append(done)\n",
    "\n",
    "        if os.path.isfile(dataset_path):\n",
    "            dataset = MDPDataset.load(dataset_path)\n",
    "            dataset.append(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                            np.asarray(terminals))\n",
    "        else:\n",
    "            dataset = MDPDataset(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                    np.asarray(terminals), discrete_action=True)\n",
    "        dataset.dump(dataset_path)\n",
    "        stats = dataset.compute_stats()\n",
    "        mean = stats['return']['mean']\n",
    "        std = stats['return']['std']\n",
    "        print(f'mean: {mean}, std: {std}')\n",
    "except ConnectionResetError:\n",
    "    print(\"Done\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Randomly generated data (optional)\n",
    "To complement the player generated data, it is possible to also generate some random data for the algorithm to train with."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate random data\n",
    "\n",
    "EPISODES = 2 # <--- increase if you want more random data. More data might slow down the training process.\n",
    "\n",
    "env_rand = Env(visible=False, level=level).env\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "\n",
    "for episode in range(0, EPISODES):\n",
    "    observation = env_rand.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env_rand.action_space.sample()\n",
    "        observation, reward, done, info = env_rand.step(action)\n",
    "        observations.append(observation)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        terminals.append(done)\n",
    "\n",
    "    if episode != 0:\n",
    "        if os.path.isfile(dataset_path_rand):\n",
    "            dataset = MDPDataset.load(dataset_path_rand)\n",
    "            dataset.append(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                           np.asarray(terminals))\n",
    "        else:\n",
    "            dataset = MDPDataset(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                 np.asarray(terminals), discrete_action=True)\n",
    "        dataset.dump(dataset_path_rand)\n",
    "        stats = dataset.compute_stats()\n",
    "        mean = stats['return']['mean']\n",
    "        std = stats['return']['std']\n",
    "        print(f'mean: {mean}, std: {std}')\n",
    "\n",
    "print(\"Done!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Use the generated data to train a policy\n",
    "Now that you have generated some data for the neural network to train with, let's begin with the training.\n",
    "For the purpose of this exercise we will use the Offline RL Python library [d3rlpy](https://github.com/takuseno/d3rlpy).\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Choosing an algorithm\n",
    "![DQN](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/dqn.PNG)\n",
    "\n",
    "#### Why we chose DQN\n",
    "The Deep Q-Network approach is known to have been able to achieve human-level control in Atari games, and it is able to learn successful policies directly from high-dimensional sensory inputs (like pixels) using end-to-end reinforcement learning which makes it ideal for our purpose. [[1]](https://www.nature.com/articles/nature14236) \n",
    "\n",
    "It uses data collected from an environment to learn and train without interacting with it.\n",
    "\n",
    "For more information on DQN, please refer to [this paper](https://www.nature.com/articles/nature14236).\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Setup the training\n",
    "Let's setup some parameters before the training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training parameters\n",
    "gamma = 0.99 # discount facor, how important future rewards are\n",
    "learning_rate = 0.0003 # to what extent the agent overrides old information with new information\n",
    "target_update_interval = 3000 # intervall of steps that the agent uses to update target network\n",
    "n_epochs = 100 # <--- change here if you want to train more / less\n",
    "test_size = 0.1\n",
    "batch_size = 2 # size of training examples utilized in one iteration\n",
    "n_frames = 1 # number of frames stacked for image observation\n",
    "n_steps = 40 # number of steps for td calculation\n",
    "use_gpu = False # usage of gpu to train\n",
    "\n",
    "print(\"Done!\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Training time!\n",
    "\n",
    "If you want to track the training with tensorboard, run the following cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start tensorboard \r\n",
    "%tensorboard --logdir runs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start the training run the next cell: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env_train = Env(visible=False, level=level, run_server=True).env\r\n",
    "\r\n",
    "dataset = getDataset()\r\n",
    "\r\n",
    "train_episodes, test_episodes = train_test_split(dataset, test_size=test_size)\r\n",
    "\r\n",
    "dqn = d3rlpy.algos.DQN(learning_rate=learning_rate, gamma=gamma, use_gpu=use_gpu,\r\n",
    "                        target_update_interval=target_update_interval, batch_size=batch_size)\r\n",
    "\r\n",
    "# train offline\r\n",
    "dqn.build_with_dataset(dataset)\r\n",
    "# set environment in scorer function\r\n",
    "evaluate_scorer = evaluate_on_environment(env_train)\r\n",
    "# evaluate algorithm on the environment\r\n",
    "rewards = evaluate_scorer(dqn)\r\n",
    "name = 'marioai_%s_%s_%s_%s_%s' % (level_name, gamma, learning_rate, target_update_interval, n_epochs)\r\n",
    "model_file = pathlib.Path(MODEL_DIR, name + \".pt\")\r\n",
    "currentMax = -100000\r\n",
    "dqn_max = copy.deepcopy(dqn)\r\n",
    "\r\n",
    "for epoch, metrics in (dqn.fitter(train_episodes, eval_episodes=test_episodes, tensorboard_dir='runs', experiment_name=name, n_epochs=n_epochs, scorers={'environment': evaluate_scorer})):\r\n",
    "    if metrics.get(\"environment\") > currentMax:\r\n",
    "        currentMax = metrics.get(\"environment\")\r\n",
    "        dqn_max.copy_q_function_from(dqn)\r\n",
    "    else:\r\n",
    "        dqn.copy_q_function_from(dqn_max)\r\n",
    "    \r\n",
    "    dqn.save_model(model_file)\r\n",
    "    if currentMax > 100: # For the purpose of the exercise the training will stop if the agent manages to complete the level\r\n",
    "        print(\"A suitable model has been found.\")\r\n",
    "        break\r\n",
    "\r\n",
    "print(\"Done!\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Validation\n",
    "Now let's see if the training did something. If the results are not as expected try recording more data or increasing the training epochs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env_show = Env(visible=True, level=level).env\r\n",
    "dqn = d3rlpy.algos.DQN()\r\n",
    "dqn.build_with_dataset(getDataset())\r\n",
    "dqn.load_model(model_file)\r\n",
    "\r\n",
    "try:\r\n",
    "    while True:\r\n",
    "        observation = env_show.reset()\r\n",
    "        done = False\r\n",
    "        total_reward = 0\r\n",
    "        while not done:\r\n",
    "            observation, reward, done, info = env_show.step(\r\n",
    "                dqn.predict([observation])[0])\r\n",
    "            total_reward += reward\r\n",
    "\r\n",
    "        print(f'finished episode, total_reward: {total_reward}')\r\n",
    "except ConnectionResetError:\r\n",
    "    print(\"Window closed.\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Offline RL vs Online RL \n",
    "\n",
    "Now we want to compare the approach from the first exercise where an online Q_Learner was used with the results we were able to achieve with the offline RL approach demonstrated above.\n",
    "\n",
    "![Gif](https://1.bp.blogspot.com/-O0FvK3zJd9w/XpXqiJduwyI/AAAAAAAAFtM/5hxzdWOoSLw5sd5vEgMsiGVJSATKx1oEgCLcBGAsYHQ/s640/OFFLINE%2BRL%2Bfig1%2B05b.gif)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Reproducibility\n",
    "To compare as fairly as possible we ran both the Online Q_Learner as well as the offline Deep Q Network until a plateau of performance has been reached.\n",
    "- The online Q_Learner was able to train for 10.000 episodes per level while being able to interact with the environment\n",
    "- The DQN was fed <1 hour (~360 episodes) of human playtime per level and was not allowed to interact with the environment while training\n",
    "- Both models were using the same reward settings\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Easy level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 232     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_q_learner_232.gif)    |\n",
    "| Deep Q Network  | 280    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_dqn_280.gif)    |\n",
    "\n",
    "With sufficient training, neither model struggles with the easy level. However, the model fed with player generated data shows better anticipation of jumps which leads to a better overall result."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Medium level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 176     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_q_learner_176.gif)    |\n",
    "| Deep Q Network  | 193    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_dqn_193.gif)    |\n",
    "\n",
    "In the medium level, both models behave similarly to the easy level. While the Online Learner seems to have a better strategy to avoid enemies, the Offline Learner has the better jumping performance leading to a better overall score as it is quicker to finish the level."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Hard level\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | -559     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_q_learner_-559.gif)    |\n",
    "| Deep Q Network  | -34    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_dqn_-34.gif)    |\n",
    "\n",
    "Neither model manages to complete the really hard level we tested them on. It is however interesting to observe the different strategies they applied. While the Q_Learner shows a very promising leap to the middle platform, failing to reach the final platform it seems to just give up and wait for the time to run out leading to a high time punishment and therefore an extremely low score. The offline trained model while still failing to complete the level, has developed a strategy to avoid the high time punishment by committing suicide as soon as possible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Reward Summary\n",
    "\n",
    "![Summary](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/level-summary.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6 Performance over 100 level\n",
    "\n",
    "// TODO -> With video maybe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion\r\n",
    "\r\n",
    "While achieving slightly better results than the online RL approach in the easy and medium level, the offline DQN approach was not able to perform any better than the online implementation in the hard level we tested them both on. Our guess would be the lack of training data as mastering a level of this complexity would require a level of understanding of the environment that the network was not able to abstract from the data we were able to provide. This also demonstrates the biggest issue with the offline RL approach: it's high dependence on good and plentiful data. \r\n",
    "Creating such data is a time-consuming and often times expensive endeavor. \r\n",
    "\r\n",
    "In the real world offline RL is often times the only sensible way to use an ML model as online learning would simply be too dangerous or slow. \r\n",
    "However, this comes with the price of the agent not being able to explore different approaches on its own meaning an often times subpar training result.\r\n",
    "Companies like Tesla have acknowledged this issue and have been training their self-driving cars in a life like simulated environment in which they reproduce difficult situations the car might encounter to explore without any real world damages. [[3]](https://youtu.be/11QXiJ8ORe8?t=3187)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "// TODO: more stuff\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Thanks](https://media4.giphy.com/media/1mssFONYwmBlJy1DAv/giphy.gif?cid=ecf05e47fq7b3e8nbn49rxb2hj1f8qy627umny603h7tsi8f&rid=giphy.gif&ct=g)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fefe5704b33e4500f5833ae6032bcef53ee790cf4ffa6448116d63501fea73e"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('marioai-iKFqD-69': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}