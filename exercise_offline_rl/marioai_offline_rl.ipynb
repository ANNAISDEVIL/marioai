{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training an agent to play Super Mario using player recorderd data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will learn how to use player generated data to train a neural network to play Super Mario. The results will be evaluated against the results from the Q_Learner exercise. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating data\n",
    "First, you will have to generate some data for the neural network to train with.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Player generated data.\r\n",
    "\r\n",
    "![Mario](https://media1.giphy.com/media/aX0RqLt2ARSW4/giphy.gif?cid=ecf05e47fnkts3fqh25tj9v8noh9vnccwo4x0ey4zpdxc7ft&rid=giphy.gif&ct=g)\r\n",
    "\r\n",
    "\r\n",
    "To achieve the best possible results, the training algorithm needs the best possible data. In this case that means player generated data.\r\n",
    "\r\n",
    "\r\n",
    "You will have the most fun playing with a USB-Controller but if you have none, you can set the following variable to ``False`` to use the keyboard:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Don't forget to run me\r\n",
    "USE_GAMEPAD = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Controls\r\n",
    "|            \t| Keyboard    \t| Xbox       \t| Playstation \t|\r\n",
    "|------------\t|-------------\t|------------\t|-------------\t|\r\n",
    "| Jump       \t| S           \t| A          \t| X           \t|\r\n",
    "| Sprint     \t| A           \t| B          \t| O           \t|\r\n",
    "| Move Right \t| Arrow Right \t| Dpad Right \t| Dpad Right  \t|\r\n",
    "| Move Left  \t| Arrow Left  \t| Dpad Left  \t| Dpad Left   \t|\r\n",
    "| Duck       \t| Arrow Down  \t| Dpad Down  \t| Dpad Down   \t|"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Level\n",
    "The level we will be using for this exercise will be a very easy one to minimize the training time.\n",
    "\n",
    "However, if you would like to try different levels, we encourage you to do so by changing the ``level`` variable below to a different one from the ``exercise_offline_rl\\levels`` folder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup the imports and global variables. Run this cell again if you encounter any import errors.\r\n",
    "import os\r\n",
    "from d3rlpy.dataset import MDPDataset\r\n",
    "from gym_setup import Env\r\n",
    "from Controller.gamepad_controller import GamepadController\r\n",
    "from Controller.keyboard_controller import KeyboardController\r\n",
    "import numpy as np\r\n",
    "from data.datasets.getDatasets import getDataset\r\n",
    "from gym_setup import Env\r\n",
    "import d3rlpy\r\n",
    "import pathlib\r\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import copy\r\n",
    "\r\n",
    "level = os.path.join(\"levels\", \"OneCliffLevel.lvl\") # <--- change here if you want to try a different level.\r\n",
    "\r\n",
    "dataset_path = os.path.join(\r\n",
    "    \"data\", \"datasets\", os.path.split(level)[1] + \".h5\")\r\n",
    "\r\n",
    "dataset_path_rand = os.path.join(\r\n",
    "    \"data\", \"datasets\", os.path.split(level)[1] + \".random.h5\")\r\n",
    "\r\n",
    "MODEL_DIR = pathlib.Path(\"data\", \"models\")\r\n",
    "if not MODEL_DIR.exists():\r\n",
    "    MODEL_DIR.mkdir(parents=True)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's play!\r\n",
    "try:\r\n",
    "    env_play = Env(visible=True, level=level).env\r\n",
    "    if USE_GAMEPAD:\r\n",
    "        controller = GamepadController(env_play)\r\n",
    "    else:\r\n",
    "        controller = KeyboardController(env_play)\r\n",
    "    while True:\r\n",
    "        observation = env_play.reset()\r\n",
    "        done = False\r\n",
    "        action = controller.read()\r\n",
    "\r\n",
    "        observations = [observation]\r\n",
    "        actions = [action]\r\n",
    "        # No reward at first time step, because no action was taken yet\r\n",
    "        rewards = [0]\r\n",
    "        terminals = [done]\r\n",
    "\r\n",
    "        while not done:\r\n",
    "            observation, reward, done, info = env_play.step(action)\r\n",
    "            action = controller.read()\r\n",
    "\r\n",
    "            observations.append(observation)\r\n",
    "            actions.append(action)\r\n",
    "            rewards.append(reward)\r\n",
    "            terminals.append(done)\r\n",
    "\r\n",
    "        if os.path.isfile(dataset_path):\r\n",
    "            dataset = MDPDataset.load(dataset_path)\r\n",
    "            dataset.append(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\r\n",
    "                            np.asarray(terminals))\r\n",
    "        else:\r\n",
    "            dataset = MDPDataset(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\r\n",
    "                                    np.asarray(terminals), discrete_action=True)\r\n",
    "        dataset.dump(dataset_path)\r\n",
    "        stats = dataset.compute_stats()\r\n",
    "        mean = stats['return']['mean']\r\n",
    "        std = stats['return']['std']\r\n",
    "        print(f'mean: {mean}, std: {std}')\r\n",
    "except ConnectionResetError:\r\n",
    "    print(\"Done\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Randomly generated data (optional)\n",
    "To complement the player generated data, it is possible and encouraged to also generate some random data for the algorithm to train with."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate random data\r\n",
    "\r\n",
    "EPISODES = 20\r\n",
    "\r\n",
    "env_rand = Env(visible=False, level=level).env\r\n",
    "\r\n",
    "observations = []\r\n",
    "actions = []\r\n",
    "rewards = []\r\n",
    "terminals = []\r\n",
    "\r\n",
    "for episode in range(0, EPISODES):\r\n",
    "    observation = env_rand.reset()\r\n",
    "    done = False\r\n",
    "\r\n",
    "    while not done:\r\n",
    "        action = env_rand.action_space.sample()\r\n",
    "        observation, reward, done, info = env_rand.step(action)\r\n",
    "        observations.append(observation)\r\n",
    "        actions.append(action)\r\n",
    "        rewards.append(reward)\r\n",
    "        terminals.append(done)\r\n",
    "\r\n",
    "    if episode % 5 == 0 and episode != 0:\r\n",
    "        if os.path.isfile(dataset_path_rand):\r\n",
    "            dataset = MDPDataset.load(dataset_path_rand)\r\n",
    "            dataset.append(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\r\n",
    "                           np.asarray(terminals))\r\n",
    "        else:\r\n",
    "            dataset = MDPDataset(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\r\n",
    "                                 np.asarray(terminals), discrete_action=True)\r\n",
    "        dataset.dump(dataset_path_rand)\r\n",
    "        stats = dataset.compute_stats()\r\n",
    "        mean = stats['return']['mean']\r\n",
    "        std = stats['return']['std']\r\n",
    "        print(f'mean: {mean}, std: {std}')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Use the generated data to train a policy\n",
    "Now that you have generated some data for the neural network to train with, let's begin with the training.\n",
    "For the purpose of this exercise we will use the Offline RL Python library d3rlpy.\n",
    "\n",
    "Our main goal was to display how good data influences the effect of an RL algorithm.\n",
    "Creating measuring and learning data for an reinforcement Learning algorithm needs a lot of ressources,\n",
    "this leads to the conclusion that you can not afford to create a lot of learning data.\n",
    "We trained our Deep Q-Network with one hour of data played by ourselfs for each map.\n",
    "Even with such a small amount of data our Mario AI learned to complete complex levels.\n",
    "To display the weight of that \"good\" data we compared it to a Q-learner which learned for 10000 episodes to train a\n",
    "a model for the network.\n",
    "\n",
    "Note: We have pregenerated some training data for your convenice which will be used in addition to your data to train the model.\n",
    "If you want to train with your own data only, go ahead and delete the data from ``exercise_offline_rl\\data\\datasets``."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Choosing an algorithm\n",
    "![DQN](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/dqn.PNG)\n",
    "\n",
    "#### Why we chose DQN? <br>\n",
    "Our main goal was to display how good data influences the effect of an RL algorithm.\n",
    "Creating measuring and learning data for a reinforcement Learning algorithm often consumes a lot of ressources,\n",
    "this leads to the conclusion that you can not afford to create a lot of learning data.\n",
    "Therefore in this exercise we try to teach an offline RL algorithm to work with a limited amount of data (1 hour of player\n",
    "created data per level) to solve complex mario levels. To compare our results we trained a qlearner for 10 000 episodes\n",
    "on the chosen levels. <br>\n",
    "Therefor we chose the DQN algorithm as our offline algorithm since it's Algorithm is based on a similiar groundwork as\n",
    "the qlearner. Additionally it has a high efficency. After recording player based data and qlearner data, we train a player\n",
    "and a qlearner model for the maps to compare their individual results with each other. <br>\n",
    "We expect our data to perform better than the qlearner data even tho it is significantly less data. This is because we\n",
    "as players only have to master the mechanics of the game and can plan ahead from the first episode we play, while the qlearner\n",
    "needs to learn everything from scratch. Since we evaluate mainly complex levels this leads the result, that the qlearner\n",
    "will need a lot of time to learn to finish the level or might not even be able to finish the level at all. Comparably\n",
    "we expect our data to have a 100% win ratio on the levels, which leads to better learning data for the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Setup the training\n",
    "Let's setup some parameters before the training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training parameters\r\n",
    "gamma = 0.99\r\n",
    "learning_rate = 0.0003\r\n",
    "target_update_interval = 3000\r\n",
    "n_epochs = 10 # <--- change here if you want to train more / less\r\n",
    "test_size = 0.1\r\n",
    "batch_size = 2\r\n",
    "n_frames = 1\r\n",
    "n_steps = 40\r\n",
    "use_gpu = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Training time!\r\n",
    "\r\n",
    "To start the training run the next cell. If you want to see the progress of your training you can adittionaly open a new terminal and run ``pipenv run board`` to see the Tensorboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env_train = Env(visible=False, level=level, run_server=True).env\r\n",
    "\r\n",
    "dataset = getDataset()\r\n",
    "\r\n",
    "train_episodes, test_episodes = train_test_split(dataset, test_size=test_size)\r\n",
    "\r\n",
    "dqn = d3rlpy.algos.DQN(learning_rate=learning_rate, gamma=gamma, use_gpu=use_gpu,\r\n",
    "                        target_update_interval=target_update_interval, batch_size=batch_size)\r\n",
    "\r\n",
    "# train offline\r\n",
    "dqn.build_with_dataset(dataset)\r\n",
    "# set environment in scorer function\r\n",
    "evaluate_scorer = evaluate_on_environment(env_train)\r\n",
    "# evaluate algorithm on the environment\r\n",
    "rewards = evaluate_scorer(dqn)\r\n",
    "name = 'marioai_%s_%s_%s_%s_%s' % (level.split('/')[-1], gamma, learning_rate, target_update_interval, n_epochs)\r\n",
    "model_file = pathlib.Path(MODEL_DIR, name + \".pt\")\r\n",
    "currentMax = -100000\r\n",
    "dqn_max = copy.deepcopy(dqn)\r\n",
    "\r\n",
    "for epoch, metrics in (dqn.fitter(train_episodes, eval_episodes=test_episodes, tensorboard_dir='runs', experiment_name=name, n_epochs=n_epochs, scorers={'environment': evaluate_scorer})):\r\n",
    "    if metrics.get(\"environment\") > currentMax:\r\n",
    "        currentMax = metrics.get(\"environment\")\r\n",
    "        dqn_max.copy_q_function_from(dqn)\r\n",
    "    else:\r\n",
    "        dqn.copy_q_function_from(dqn_max)\r\n",
    "\r\n",
    "    dqn.save_model(model_file)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 See what worked\n",
    "Now let's see if the training did something:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env_show = Env(visible=True, level=level).env\r\n",
    "dqn = d3rlpy.algos.DQN()\r\n",
    "dqn.build_with_dataset(getDataset())\r\n",
    "dqn.load_model(model_file)\r\n",
    "\r\n",
    "while True:\r\n",
    "    observation = env_show.reset()\r\n",
    "    done = False\r\n",
    "    total_reward = 0\r\n",
    "    while not done:\r\n",
    "        observation, reward, done, info = env_show.step(\r\n",
    "            dqn.predict([observation])[0])\r\n",
    "        total_reward += reward\r\n",
    "\r\n",
    "    print(f'finished episode, total_reward: {total_reward}')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Offline RL vs Online RL \n",
    "Now we want to compare the approach from exercise 1 where an online Q_Learner was used with the results one can get with the offline RL approach."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.0 Reproducibility\n",
    "To compare as fairly as possible we ran both the Online Q_Learner as well as the offline Deep Q Network until a plateau of performance has been reached."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Easy level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 232     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_q_learner_232.gif)    |\n",
    "| Deep Q Network  | 280    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/rough_terrain_dqn_280.gif)    |\n",
    "\n",
    "With sufficient training, neither model struggles with the easy level. However, the model fed with player generated data show better anticipation of jumps which leads to a better overall result."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Medium level\n",
    "\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | 176     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_q_learner_176.gif)    |\n",
    "| Deep Q Network  | 193    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/cliff_and_enemies_dqn_193.gif)    |\n",
    "\n",
    "In the medium level, both models behave similarly to the easy level. While the Online Learner seems to have a better strategy to avoid enemies, the Offline Learner has the better jumping performance leading to a better overall score as it is quicker to finish the level."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Hard level\n",
    "|  | Reward | Video |\n",
    "| -------- | -------- | -------- |\n",
    "| Q_Learner  | -559     | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_q_learner_-559.gif)    |\n",
    "| Deep Q Network  | -34    | ![Gif](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/climb_dqn_-34.gif)    |\n",
    "\n",
    "Neither model manages to complete the really hard level we tested them on. It is however interesting to observe the different strategies they applied. While the Q_Learner shows a very promising leap to the middle platform, failing to reach the final platform it seems to just give up and wait for the time to run out leading to a high time punishment and therefore an extremely low score. The offline trained model while still failing to complete the level, has developed a strategy to avoid the high time punishment by committing suicide as soon as possible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Summary\n",
    "\n",
    "![Summary](https://raw.githubusercontent.com/koerners/marioai/master/exercise_offline_rl/data/jupyter/level-summary.png)\n",
    "\n",
    "\n",
    "Both the data of our played episodes aswell as the data of the qlearner had the same reward settings. Our approach\n",
    "for the rewards was simple is best. The goal was to complete complex levels and see how good it performs with a small amount\n",
    "of good data compared to a (comparably) big amount of qlearning data (which itself had to learn the level first).\n",
    "With a very good but small dataset we now had to trained the network with each datasets for 100 epochs. Both had once again\n",
    "the same hyperparameters for the training to create a fair comparison. Our data performed much better tho. This is because\n",
    "we reached the goal from the beginning on and tried out different ways to do so, giving the Network the possibility to learn\n",
    "from different possible actions that lead to the goal. Meanwhile the qlearner itself had problems completing the levels in\n",
    "10 000 episodes since they were \"too\" complex to learn in such a short time without any reference data. This is made visible\n",
    "in all 3 grphics. At some point the qlearner completed the level but didn't optimize it's Qtable yet, which leads to our\n",
    "data resulting in having better Rewards in every level. The graph of the hard level has negative rewards for both our data\n",
    "model aswell as the qlearner data model, since both did not manage to complete the level. The level was too complex for\n",
    "the data since it needed alot of climbing and even tho the data for that level was good aswell the network didn't manage to\n",
    "learn to climb over a high wall."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Performance over 100 level\n",
    "\n",
    "// TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "// TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Thanks](https://media4.giphy.com/media/1mssFONYwmBlJy1DAv/giphy.gif?cid=ecf05e47fq7b3e8nbn49rxb2hj1f8qy627umny603h7tsi8f&rid=giphy.gif&ct=g)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fefe5704b33e4500f5833ae6032bcef53ee790cf4ffa6448116d63501fea73e"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('marioai-iKFqD-69': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}