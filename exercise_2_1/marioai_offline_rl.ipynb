{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training an agent to play Super Mario using player recorderd data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will learn how to use player generated data to train a neural network to play Super Mario. The results will be evaluated against the results from the Q_Learner exercise. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating data\n",
    "First, you will have to generate some data for the neural network to train with.\n",
    "You will have the most fun playing with a USB-Controller but if you have none, you can set the following variable to false to use the keyboard:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "USE_GAMEPAD = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "// TODO: Steuerung erklÃ¤ren"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import subprocess\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "from gym_setup import Env\n",
    "from gamepad_controller import GamepadController\n",
    "from keyboard_controller import KeyboardController\n",
    "import numpy as np\n",
    "\n",
    "level = os.path.join(\"levels\", \"RoughTerrainLevel.lvl\") # TODO: Use a very easy level for this exercise\n",
    "\n",
    "try:\n",
    "    with subprocess.Popen(['java', '-jar', 'server.jar'], shell=True) as server:\n",
    "        env = Env(visible=True, port=8080, level=level, run_server=False).env\n",
    "        if USE_GAMEPAD:\n",
    "            controller = GamepadController(env)\n",
    "        else:\n",
    "            controller = KeyboardController(env)\n",
    "        while True:\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            action = controller.read()\n",
    "\n",
    "            observations = [observation]\n",
    "            actions = [action]\n",
    "            rewards = [0]  # No reward at first time step, because no action was taken yet\n",
    "            terminals = [done]\n",
    "\n",
    "            while not done:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                action = controller.read()\n",
    "\n",
    "                observations.append(observation)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                terminals.append(done)\n",
    "\n",
    "            dataset_path = os.path.join(\"data\", \"datasets\", os.path.split(level_path)[1] + \".h5\")\n",
    "            if os.path.isfile(dataset_path):\n",
    "                dataset = MDPDataset.load(dataset_path)\n",
    "                dataset.append(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                np.asarray(terminals))\n",
    "            else:\n",
    "                dataset = MDPDataset(np.asarray(observations), np.asarray(actions), np.asarray(rewards),\n",
    "                                        np.asarray(terminals), discrete_action=True)\n",
    "            dataset.dump(dataset_path)\n",
    "            stats = dataset.compute_stats()\n",
    "            mean = stats['return']['mean']\n",
    "            std = stats['return']['std']\n",
    "            print(f'mean: {mean}, std: {std}')\n",
    "except ConnectionResetError:\n",
    "    # Finish\n",
    "    pass\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Use the generated data to train a policy\n",
    "Now that you have generated some data for the neural network to train with, let's begin with the training.\n",
    "For the purpose of this exercise we will use the Offline RL Python library d3rlpy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Choosing an algorithm\n",
    "// TODO: Why we chose DQN\n",
    "![DQN](data/dqn.PNG)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Setup the training\n",
    "Let's setup some parameters before the training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from data.datasets.getDatasets import getDataset\n",
    "from gym_setup import Env\n",
    "import d3rlpy\n",
    "import pathlib\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import socket\n",
    "from contextlib import closing\n",
    "\n",
    "MODEL_DIR = pathlib.Path(\"data\", \"models\")\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir(parents=True)\n",
    "\n",
    "# Environment settings\n",
    "port = 8081\n",
    "run_server = True\n",
    "visible = False\n",
    "\n",
    "# Training parameters\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0003\n",
    "target_update_interval = 3000\n",
    "n_epochs = 1000\n",
    "test_size = 0.1\n",
    "batch_size = 2\n",
    "n_frames = 1\n",
    "n_steps = 40\n",
    "use_gpu = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### 2.3 Training time!\n",
    "To start the training run the next cell. If you want to see the progress of your training you can adittionaly open a new terminal and run ``pipenv run board`` to see the Tensorboard."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    env = Env(visible=visible, port=find_free_port(), level=level, run_server=run_server).env\n",
    "\n",
    "    dataset = getDataset()\n",
    "\n",
    "    train_episodes, test_episodes = train_test_split(dataset, test_size=test_size)\n",
    "\n",
    "    dqn = d3rlpy.algos.DQN(learning_rate=learning_rate, gamma=gamma, use_gpu=use_gpu,\n",
    "                           target_update_interval=target_update_interval, batch_size=batch_size)\n",
    "\n",
    "    # train offline\n",
    "    dqn.build_with_dataset(dataset)\n",
    "    # set environment in scorer function\n",
    "    evaluate_scorer = evaluate_on_environment(env)\n",
    "    # evaluate algorithm on the environment\n",
    "    rewards = evaluate_scorer(dqn)\n",
    "    name = 'marioai_%s_%s_%s_%s_%s' % (level.split('/')[-1], gamma, learning_rate, target_update_interval, n_epochs)\n",
    "    currentMax = -100000\n",
    "    dqn_max = copy.deepcopy(dqn)\n",
    "\n",
    "    for epoch, metrics in (dqn.fitter(train_episodes, eval_episodes=test_episodes, tensorboard_dir='runs', experiment_name=name, n_epochs=n_epochs, scorers={'environment': evaluate_scorer})):\n",
    "        if metrics.get(\"environment\") > currentMax:\n",
    "            currentMax = metrics.get(\"environment\")\n",
    "            dqn_max.copy_q_function_from(dqn)\n",
    "        else:\n",
    "            dqn.copy_q_function_from(dqn_max)\n",
    "\n",
    "    model_file = pathlib.Path(MODEL_DIR, name + \".pt\")\n",
    "    dqn.save_model(model_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### 2.4 See what worked\n",
    "Now let's see if the training did something:"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = Env(visible=True, level=level, port=8082).env\n",
    "dqn = DQN()\n",
    "dqn.build_with_dataset(getDataset())\n",
    "dqn.load_model('data/models/model.pt')\n",
    "\n",
    "while True:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(dqn.predict([observation])[0])\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f'finished episode, total_reward: {total_reward}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Offline RL vs Online RL \n",
    "Now we want to compare the results from exercise 1 where and online Q_Learner was used with the results one can get with the offline RL approach."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Easy level"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Medium Level"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Hard Level"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Conclusion"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}