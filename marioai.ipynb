{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eH1YJeonXKg"
   },
   "source": [
    "# Training an Agent to play Super Mario\n",
    "In this exercise you are going to train a Q-Learning agent on the ```gym-marioai``` domain.  \n",
    "gym_marioai provides a python interface to interact with the MarioAI engine in a comfortable way. The engine itself is implemented in java, and the ```.jar``` of the engine needs to be started separately.  \n",
    "\n",
    "### Installation\n",
    "Requirements: Java 8 runtime environment, python 3.?  \n",
    "You will be provided with both the .jar and the gym-marioai python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TbJVNqroiOm"
   },
   "outputs": [],
   "source": [
    "# install the gym-environment\n",
    "# navigate to the source folder, then run:\n",
    "# pip install ./path/to/gym-marioai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH5X6wm7pQ7x"
   },
   "source": [
    "### Running the MarioAI server:\n",
    "navigate to the folder containing ```marioai-server.jar```, then run the following:  \n",
    "```java -jar ./marioai-server.jar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the server\r\n",
    "To run the server, run the following cell. It will launch the jar containing the Java engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen\r\n",
    "server_process = Popen(\r\n",
    "    ['java', '-jar', 'marioai-proto-interface/target/marioai-proto-interface-0.1-SNAPSHOT-jar-with-dependencies.jar'])\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the MarioAI server: (this also happens automatically by closing the render window)\r\n",
    "To kill the server process comment in the following cell and run it. Do not do this yet though ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_process.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoZ2eqZYpv2V"
   },
   "source": [
    "### python-client demo setup:\n",
    "make sure the demo is running..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_XSGD4ztdYy"
   },
   "source": [
    "## Representation of the Q-table\n",
    "You will experience some of the shortcomings of tabular reinforcement learning methods. With marioai, the observation space will be very large, resulting in\n",
    "- longer training duration (no interpolation of the policy between similar observations, each state needs to be explored separately)\n",
    "- large amount of memory required to store the Q-table, if implemented naively\n",
    "\n",
    "However, we can assume that only a subset of the observation space will be visited.  \n",
    "\n",
    "Task: Implement a representation of the Q-table that stores observations 'on-demand'.  \n",
    "Optional: Think of a way to store and reuse the trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqbAleu6wXh2"
   },
   "source": [
    "## Training a Q-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import gym\r\n",
    "import gym_marioai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\r\n",
    "    \"\"\"\r\n",
    "    data structure to store the Q function for hashable state representations\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, n_actions, initial_capacity=100):\r\n",
    "        self.capacity = initial_capacity\r\n",
    "        self.num_states = 0\r\n",
    "        self.state_index_map = {}\r\n",
    "        self.table = np.zeros([initial_capacity, n_actions])\r\n",
    "\r\n",
    "    def __contains__(self, state):\r\n",
    "        \"\"\" 'in' operator \"\"\"\r\n",
    "        return state in self.state_index_map\r\n",
    "\r\n",
    "    #def __len__(self):\r\n",
    "    #    return self.num_states\r\n",
    "\r\n",
    "    def __getitem__(self, state):\r\n",
    "        \"\"\" access state directly using [] notation \"\"\"\r\n",
    "        if state not in self.state_index_map:\r\n",
    "            self.init_state(state)\r\n",
    "\r\n",
    "        return self.table[self.state_index_map[state]]\r\n",
    "\r\n",
    "    def init_state(self, state):\r\n",
    "        if self.num_states == self.capacity:\r\n",
    "            # need to increase capacity\r\n",
    "            self.table = np.concatenate(\r\n",
    "                (self.table, np.zeros_like(self.table)))\r\n",
    "            self.capacity *= 2\r\n",
    "        self.state_index_map[state] = self.num_states\r\n",
    "        self.num_states += 1\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\r\n",
    "#   Training Parameters\r\n",
    "#####################################\r\n",
    "n_episodes = 5000\r\n",
    "alpha = 0.1\r\n",
    "gamma = 0.99\r\n",
    "lmbda = 0.75\r\n",
    "epsilon_start = 0.5\r\n",
    "epsilon_end = 0.001\r\n",
    "epsilon_decay_length = n_episodes / 2\r\n",
    "decay_step = (epsilon_start - epsilon_end) / epsilon_decay_length\r\n",
    "\r\n",
    "SAVE_FREQ = 100\r\n",
    "\r\n",
    "#####################################\r\n",
    "#   Environment/Reward Settings\r\n",
    "#####################################\r\n",
    "level = 'earlyCliffLevel'\r\n",
    "path = None\r\n",
    "\r\n",
    "if level == 'cliffLevel':\r\n",
    "    path = gym_marioai.levels.cliff_level\r\n",
    "if level == 'oneCliffLevel':\r\n",
    "    path = gym_marioai.levels.one_cliff_level\r\n",
    "if level == 'earlyCliffLevel':\r\n",
    "    path = gym_marioai.levels.early_cliff_level\r\n",
    "\r\n",
    "trace = 3\r\n",
    "rf_width = 20\r\n",
    "rf_height = 10\r\n",
    "prog = 1\r\n",
    "timestep = -1\r\n",
    "cliff = 1000\r\n",
    "win = -100\r\n",
    "dead = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\r\n",
    "    \"\"\"\r\n",
    "    training\r\n",
    "    \"\"\"\r\n",
    "    log_path = f'{level}_{rf_width}x{rf_height}_trace{trace}_prog{prog}_cliff{cliff}_win{win}_dead{dead}-0'\r\n",
    "    # logger = Logger(log_path)\r\n",
    "    # collect some training statistics\r\n",
    "    all_rewards = np.zeros([SAVE_FREQ])\r\n",
    "    all_wins = np.zeros([SAVE_FREQ])\r\n",
    "    all_steps = np.zeros([SAVE_FREQ])\r\n",
    "    all_gap_jumps = np.zeros([SAVE_FREQ])\r\n",
    "\r\n",
    "    ###################################\r\n",
    "    #       environment setup\r\n",
    "    ###################################\r\n",
    "    reward_settings = gym_marioai.RewardSettings(\r\n",
    "        progress=prog, timestep=timestep, cliff=cliff, win=win, dead=dead)\r\n",
    "    env = gym.make('Marioai-v0', render=False,\r\n",
    "                   level_path=path,\r\n",
    "                   reward_settings=reward_settings,\r\n",
    "                   compact_observation=True,\r\n",
    "                   trace_length=trace,\r\n",
    "                   rf_width=rf_width, rf_height=rf_height)\r\n",
    "\r\n",
    "    try:\r\n",
    "        ####################################\r\n",
    "        #       Q-learner setup\r\n",
    "        #####################################\r\n",
    "        Q = QTable(env.n_actions, 128)\r\n",
    "        etrace = {}\r\n",
    "\r\n",
    "        ####################################\r\n",
    "        #      Training Loop\r\n",
    "        ####################################\r\n",
    "        for e in range(n_episodes+1):\r\n",
    "            done = False\r\n",
    "            info = {}\r\n",
    "            total_reward = 0\r\n",
    "            steps = 0\r\n",
    "\r\n",
    "            # exponential decay\r\n",
    "            epsilon = (epsilon_end / epsilon_start) ** (e /\r\n",
    "                                                        n_episodes) * epsilon_start\r\n",
    "\r\n",
    "            state = env.reset()\r\n",
    "            #state = tuple([s.tobytes() for s in state])\r\n",
    "            # choose a' from a Policy derived from Q\r\n",
    "            if np.random.rand() < epsilon:\r\n",
    "                action = env.action_space.sample()\r\n",
    "            else:\r\n",
    "                action = int(np.argmax(Q[state]))  # greedy\r\n",
    "\r\n",
    "            while not done:\r\n",
    "                next_state, reward, done, info = env.step(action)\r\n",
    "                #next_state = tuple([s.tobytes() for s in next_state])\r\n",
    "                total_reward += reward\r\n",
    "\r\n",
    "                # choose a' from a Policy derived from Q\r\n",
    "                best_next_action = int(np.argmax(Q[next_state]))  # greedy\r\n",
    "                if np.random.rand() < epsilon:\r\n",
    "                    next_action = env.action_space.sample()\r\n",
    "                else:\r\n",
    "                    next_action = best_next_action\r\n",
    "\r\n",
    "                # calculate the TD error\r\n",
    "                td_error = reward + gamma * \\\r\n",
    "                    Q[next_state][best_next_action] - Q[state][action]\r\n",
    "\r\n",
    "                # reset eligibility trace for (s,a) using replacing strategy\r\n",
    "                etrace[(state, action)] = 1\r\n",
    "\r\n",
    "                # perform Q update\r\n",
    "                if best_next_action == next_action:\r\n",
    "                    for (s, a), eligibility in etrace.items():\r\n",
    "                        Q[s][a] += alpha * eligibility * td_error\r\n",
    "                        etrace[(s, a)] *= gamma * lmbda\r\n",
    "                else:\r\n",
    "                    for (s, a), eligibility in etrace.items():\r\n",
    "                        Q[s][a] += alpha * eligibility * td_error\r\n",
    "                    etrace = {}\r\n",
    "\r\n",
    "                steps += 1\r\n",
    "                action = next_action\r\n",
    "                state = next_state\r\n",
    "\r\n",
    "            # episode finished\r\n",
    "            # logger.append(total_reward, info['steps'], info['win'])\r\n",
    "\r\n",
    "            all_rewards[e % SAVE_FREQ] = total_reward\r\n",
    "            all_wins[e % SAVE_FREQ] = 1 if info['win'] else 0\r\n",
    "            all_steps[e % SAVE_FREQ] = info['steps']\r\n",
    "            all_gap_jumps[e % SAVE_FREQ] = info['cliff_jumps']\r\n",
    "\r\n",
    "            if e % SAVE_FREQ == 0 and e > 0:\r\n",
    "                # logger.save()\r\n",
    "                # logger.save_model(Q)\r\n",
    "                print(f'Episode: {e}', 'Eps: {epsilon:.3f}', f'Avg Reward: {all_rewards.mean():>4.2f}',\r\n",
    "                    f'Avg steps: {all_steps.mean():>4.2f}', f'Win% : {all_wins.mean():3.2f}',\r\n",
    "                    f'Cliff jumps: {all_gap_jumps.mean():.1f}', f'States seen: {Q.num_states}', end='\\r')\r\n",
    "        env.teardown()\r\n",
    "        return Q\r\n",
    "    except KeyboardInterrupt:\r\n",
    "        env.teardown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "Q = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aEJP8bpixMpC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished episode. reward: 10158.00\t steps: 842.00\twin: False\t gap jumps: 11\n",
      "closing socket connection...\n",
      "socket connection closed successfully.\n",
      "socket disconnected.\n"
     ]
    }
   ],
   "source": [
    "reward_settings = gym_marioai.RewardSettings(progress=prog, timestep=timestep,\r\n",
    "                                             cliff=cliff, win=win, dead=dead)\r\n",
    "env = gym.make('Marioai-v0', render=True,\r\n",
    "               level_path=path,\r\n",
    "               reward_settings=reward_settings,\r\n",
    "               compact_observation=True,\r\n",
    "               trace_length=trace,\r\n",
    "               rf_width=rf_width, rf_height=rf_height)\r\n",
    "\r\n",
    "try:\r\n",
    "    while True:\r\n",
    "        done = False\r\n",
    "        info = {}\r\n",
    "        total_reward = 0\r\n",
    "        steps = 0\r\n",
    "        state = env.reset()\r\n",
    "\r\n",
    "        while not done:\r\n",
    "            action = int(np.argmax(Q[state]))  # greedy\r\n",
    "            state, reward, done, info = env.step(action)\r\n",
    "            total_reward += reward\r\n",
    "            steps += 1\r\n",
    "\r\n",
    "        print(f'finished episode. reward: {total_reward:4.2f}\\t steps: {steps:4.2f}\\t'\r\n",
    "            f'win: {info[\"win\"]}\\t gap jumps: {info[\"cliff_jumps\"]}')\r\n",
    "except KeyboardInterrupt:\r\n",
    "    env.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bHte0ALwom6"
   },
   "source": [
    "## Plotting the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmlb_WctwxZn"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_process.kill()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "marioai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}