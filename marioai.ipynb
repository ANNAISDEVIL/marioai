{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eH1YJeonXKg"
   },
   "source": [
    "# Training an Agent to play Super Mario\n",
    "In this exercise you are going to train a Q-Learning agent on the ```gym-marioai``` domain.  \n",
    "gym_marioai provides a python interface to interact with the MarioAI engine in a comfortable way. The engine itself is implemented in java, and the ```.jar``` of the engine needs to be started separately.  \n",
    "\n",
    "### Installation\n",
    "Requirements: Java 8 runtime environment, python 3.?  \n",
    "You will be provided with both the .jar and the gym-marioai python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TbJVNqroiOm"
   },
   "outputs": [],
   "source": [
    "# install the gym-environment\n",
    "# navigate to the source folder, then run:\n",
    "# pip install ./path/to/gym-marioai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.1\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rMUPog_-zzIV"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_marioai\n",
    "import numpy as np\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH5X6wm7pQ7x"
   },
   "source": [
    "### Running the MarioAI server:\n",
    "navigate to the folder containing ```marioai-server.jar```, then run the following:  \n",
    "```java -jar ./marioai-server.jar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoZ2eqZYpv2V"
   },
   "source": [
    "### python-client demo setup:\n",
    "make sure the demo is running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9pVKAOrup20N"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4bd7105199ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/envs/mario_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mstate_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/protobuf_socket.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m\"\"\" receive, parse and return a protobuf message \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# parse the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"socket connection broken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize the environment\n",
    "env = gym.make('Marioai-v0', render=True, level_path=gym_marioai.levels.cliff_level)\n",
    "\n",
    "# run random episodes\n",
    "for episode in range(2):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f'finished episode {episode}, total_reward: {total_reward}')\n",
    "\n",
    "print('finished demo')\n",
    "env.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_XSGD4ztdYy"
   },
   "source": [
    "## Representation of the Q-table\n",
    "You will experience some of the shortcomings of tabular reinforcement learning methods. With marioai, the observation space will be very large, resulting in\n",
    "- longer training duration (no interpolation of the policy between similar observations, each state needs to be explored separately)\n",
    "- large amount of memory required to store the Q-table, if implemented naively\n",
    "\n",
    "However, we can assume that only a subset of the observation space will be visited.  \n",
    "\n",
    "Task: Implement a representation of the Q-table that stores observations 'on-demand'.  \n",
    "Optional: Think of a way to store and reuse the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b35Zp1yNv8x0"
   },
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    \"\"\"\n",
    "    data structure to store the Q function for hashable state representations\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions, initial_capacity=100):\n",
    "        self.capacity = initial_capacity\n",
    "        self.num_states = 0\n",
    "        self.state_index_map = {}\n",
    "        self.table = np.zeros([initial_capacity, n_actions])\n",
    "\n",
    "    def __getitem__(self, state):\n",
    "        \"\"\" access state directly using [] notation \"\"\"\n",
    "        if state not in self.state_index_map:\n",
    "            self._init_state(state)\n",
    "        return self.table[self.state_index_map[state]]\n",
    "\n",
    "    def _init_state(self, state):\n",
    "        if self.num_states == self.capacity:\n",
    "            self.table = np.concatenate((self.table, np.zeros_like(self.table)))\n",
    "            self.capacity *= 2\n",
    "\n",
    "        self.state_index_map[state] = self.num_states\n",
    "        self.num_states += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqbAleu6wXh2"
   },
   "source": [
    "## Training a Q-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#   Training Parameters\n",
    "#####################################\n",
    "n_episodes = 15000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "lmbda = 0.75\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.001\n",
    "epsilon_decay_length = n_episodes * 0.8\n",
    "epsilon_slope = (epsilon_end - epsilon_start) / epsilon_decay_length\n",
    "\n",
    "#####################################\n",
    "#   Environment/Reward Settings\n",
    "#####################################\n",
    "trace = 2\n",
    "rf_width = 20\n",
    "rf_height = 10\n",
    "prog = 1\n",
    "timestep = -1\n",
    "cliff = 1000\n",
    "win = -20\n",
    "dead = -10\n",
    "path = gym_marioai.levels.one_cliff_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mXs0iQ1A0yt7"
   },
   "outputs": [],
   "source": [
    "SAVE_FREQ = 100\n",
    "\n",
    "def eps_greedy(state, env, Q, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return int(np.argmax(Q[state]))\n",
    "\n",
    "    \n",
    "def train():\n",
    "    \"\"\"\n",
    "    Q Learning with epsilon decay and (replacing) eligibility traces\n",
    "    \"\"\"\n",
    "    #log_path = f'{level}_{rf_width}x{rf_height}_trace{trace}_prog{prog}_cliff{cliff}_win{win}_dead{dead}-0'\n",
    "    #logger = Logger(log_path)\n",
    "    # collect some training statistics\n",
    "    all_rewards = np.zeros([SAVE_FREQ])\n",
    "    all_wins = np.zeros([SAVE_FREQ])\n",
    "    all_steps = np.zeros([SAVE_FREQ])\n",
    "    all_gap_jumps = np.zeros([SAVE_FREQ])\n",
    "\n",
    "    ###################################\n",
    "    #       environment setup\n",
    "    ###################################\n",
    "    reward_settings = gym_marioai.RewardSettings(progress=prog, timestep=timestep, cliff=cliff, win=win, dead=dead)\n",
    "    env = gym.make('Marioai-v0', render=False,\n",
    "                   level_path=path,\n",
    "                   reward_settings=reward_settings,\n",
    "                   compact_observation=True,\n",
    "                   trace_length=trace,\n",
    "                   rf_width=rf_width, rf_height=rf_height)\n",
    "\n",
    "    ####################################\n",
    "    #       Q-learner setup\n",
    "    #####################################\n",
    "    Q = QTable(env.n_actions, 128)\n",
    "    etrace = {}\n",
    "\n",
    "    ####################################\n",
    "    #      Training Loop\n",
    "    ####################################\n",
    "    for e in range(n_episodes+1):\n",
    "        done = False\n",
    "        info = {}\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        # exponential decay\n",
    "        #epsilon = (epsilon_end / epsilon_start) ** (e / n_episodes) * epsilon_start\n",
    "        epsilon = max(epsilon_start + e * epsilon_slope, epsilon_end)\n",
    "\n",
    "        state = env.reset()\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = int(np.argmax(Q[state]))\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # choose a' from a Policy derived from Q\n",
    "            best_next_action = int(np.argmax(Q[next_state]))  # greedy\n",
    "            if np.random.rand() < epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = int(np.argmax(Q[state]))\n",
    "            \n",
    "            # calculate the TD error\n",
    "            td_error = reward + gamma * Q[next_state][best_next_action] - Q[state][action]\n",
    "\n",
    "            # reset eligibility trace for (s,a) using replacing strategy\n",
    "            etrace[(state, action)] = 1\n",
    "\n",
    "            # perform Q update\n",
    "            if best_next_action == next_action:\n",
    "                for (s, a), eligibility in etrace.items():\n",
    "                    Q[s][a] += alpha * eligibility * td_error\n",
    "                    etrace[(s, a)] *= gamma * lmbda\n",
    "            else:\n",
    "                for (s, a), eligibility in etrace.items():\n",
    "                    Q[s][a] += alpha * eligibility * td_error\n",
    "                etrace = {}\n",
    "\n",
    "            steps += 1\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "        all_rewards[e % SAVE_FREQ] = total_reward\n",
    "        all_wins[e % SAVE_FREQ] = 1 if info['win'] else 0\n",
    "        all_steps[e % SAVE_FREQ] = info['steps']\n",
    "        all_gap_jumps[e % SAVE_FREQ] = info['cliff_jumps']\n",
    "\n",
    "        if e % SAVE_FREQ == 0 and e > 0:\n",
    "            print(f'finished #{e}. eps: {epsilon:.3f} avg_R: {all_rewards.mean():>4.2f} '\n",
    "                  f'avg_steps: {all_steps.mean():>4.2f} '\n",
    "                  f'win_rate: {all_wins.mean():3.2f} gap_jumps: {all_gap_jumps.mean():.1f} '\n",
    "                  f'states: {Q.num_states}')\n",
    "    print('training finished.')\n",
    "    env.teardown()\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished #100. eps: 0.496 avg_R: -268.02 avg_steps: 296.82 win_rate: 0.01 gap_jumps: 0.0 states: 791\n",
      "finished #200. eps: 0.492 avg_R: -197.65 avg_steps: 246.20 win_rate: 0.03 gap_jumps: 0.1 states: 926\n",
      "finished #300. eps: 0.488 avg_R: -160.23 avg_steps: 208.84 win_rate: 0.03 gap_jumps: 0.1 states: 979\n",
      "finished #400. eps: 0.483 avg_R: -71.88 avg_steps: 160.12 win_rate: 0.07 gap_jumps: 0.1 states: 1035\n",
      "finished #500. eps: 0.479 avg_R: -150.44 avg_steps: 208.90 win_rate: 0.05 gap_jumps: 0.1 states: 1064\n",
      "finished #600. eps: 0.475 avg_R: -56.20 avg_steps: 233.75 win_rate: 0.13 gap_jumps: 0.2 states: 1086\n",
      "finished #700. eps: 0.471 avg_R: -13.96 avg_steps: 182.11 win_rate: 0.08 gap_jumps: 0.2 states: 1104\n",
      "finished #800. eps: 0.467 avg_R: -21.78 avg_steps: 180.02 win_rate: 0.11 gap_jumps: 0.2 states: 1124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9ba6cbbc7524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-00c0121bc3ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/envs/mario_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mstate_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_action_until_new_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mstate_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mstate_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 273.11867125,  198.69165036,  257.80384889,  235.5508301 ,\n",
       "         994.88994152,  184.15786209,  119.20277414,  189.28542897,\n",
       "         164.92359604],\n",
       "       [ 821.57713453,  958.10053996,  944.98641802,  767.18062244,\n",
       "         941.96757806,  960.25557447,  945.05896179,  970.95820646,\n",
       "         946.27578999],\n",
       "       [ 541.46669231,  557.83105983,  945.83589326,  962.07512853,\n",
       "         887.95144566,  602.16231956,  966.0081283 ,  990.89349073,\n",
       "         916.69277353],\n",
       "       [ 291.89122076,  353.19764055,  379.72456276,  949.47176187,\n",
       "         289.47465368,  598.76241304,  493.98726499,  339.41658906,\n",
       "         299.60595278],\n",
       "       [ 256.86752917,  326.5520689 ,  314.67358927,  989.17271385,\n",
       "         208.80959422,  339.16957282,  259.85740258,  295.24205572,\n",
       "         333.56878072],\n",
       "       [ 223.37780599,  278.20134585,  176.53362895, 1002.55647602,\n",
       "         134.45094642,  204.54100502,  244.95155386,  204.71361493,\n",
       "         233.31677016],\n",
       "       [ 256.9199174 ,  251.67220108,  133.15753685, 1012.55653972,\n",
       "         264.97874855,  219.92880692,  102.53646568,  171.24939122,\n",
       "         221.6511092 ],\n",
       "       [ 112.9228983 ,   42.18794868,   41.04570098,  498.59422542,\n",
       "         125.61930567,   52.40287883,  141.80493274,  154.5440544 ,\n",
       "          99.34781178],\n",
       "       [  99.94293209,   72.90742177,   78.04066841,  471.48608596,\n",
       "          23.64930676,   40.01839908,  122.59822997,   79.13487084,\n",
       "          44.34289   ],\n",
       "       [  96.74342405,  161.19479103,   27.11819542,  546.55898952,\n",
       "          29.43554286,   73.52237192,   67.5939081 ,  110.17912231,\n",
       "         117.78861996]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.table[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aEJP8bpixMpC"
   },
   "outputs": [
    {
     "ename": "ConnectionResetError",
     "evalue": "[Errno 104] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2eb41f636b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/envs/mario_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mstate_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/protobuf_socket.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m\"\"\" receive, parse and return a protobuf message \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# parse the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"socket connection broken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "reward_settings = gym_marioai.RewardSettings(progress=prog, timestep=timestep,\n",
    "                                             cliff=cliff, win=win, dead=dead)\n",
    "env = gym.make('Marioai-v0', render=True,\n",
    "               level_path=path,\n",
    "               reward_settings=reward_settings,\n",
    "               compact_observation=True,\n",
    "               trace_length=trace,\n",
    "               rf_width=rf_width, rf_height=rf_height)\n",
    "\n",
    "while True:\n",
    "    done = False\n",
    "    info = {}\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = int(np.argmax(Q[state]))  # greedy\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    print(f'finished episode. reward: {total_reward:4.2f}\\t steps: {steps:4.2f}\\t'\n",
    "          f'win: {info[\"win\"]}\\t gap jumps: {info[\"cliff_jumps\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bHte0ALwom6"
   },
   "source": [
    "## Plotting the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmlb_WctwxZn"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "marioai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
