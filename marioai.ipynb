{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eH1YJeonXKg"
   },
   "source": [
    "# Training an Agent to play Super Mario\n",
    "In this exercise you are going to train a Q-Learning agent on the ```gym-marioai``` domain.  \n",
    "gym_marioai provides a python interface to interact with the MarioAI engine in a comfortable way. The engine itself is implemented in java, and the ```.jar``` of the engine needs to be started separately.  \n",
    "\n",
    "### Installation\n",
    "Requirements: Java 8 runtime environment, python 3.?  \n",
    "You will be provided with both the .jar and the gym-marioai python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TbJVNqroiOm"
   },
   "outputs": [],
   "source": [
    "# install the gym-environment\n",
    "# navigate to the source folder, then run:\n",
    "# pip install ./path/to/gym-marioai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rMUPog_-zzIV"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_marioai\n",
    "import numpy as np\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH5X6wm7pQ7x"
   },
   "source": [
    "### Running the MarioAI server:\n",
    "navigate to the folder containing ```marioai-server.jar```, then run the following:  \n",
    "```java -jar ./marioai-server.jar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoZ2eqZYpv2V"
   },
   "source": [
    "### python-client demo setup:\n",
    "make sure the demo is running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9pVKAOrup20N"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "socket connection broken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4bd7105199ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/envs/mario_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mstate_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/protobuf_socket.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"socket connection broken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtotal_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DecodeVarint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: socket connection broken"
     ]
    }
   ],
   "source": [
    "# initialize the environment\n",
    "env = gym.make('Marioai-v0', render=True, level_path=gym_marioai.levels.cliff_level)\n",
    "\n",
    "# run random episodes\n",
    "for episode in range(2):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f'finished episode {episode}, total_reward: {total_reward}')\n",
    "\n",
    "print('finished demo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_XSGD4ztdYy"
   },
   "source": [
    "## Representation of the Q-table\n",
    "You will experience some of the shortcomings of tabular reinforcement learning methods. With marioai, the observation space will be very large, resulting in\n",
    "- longer training duration (no interpolation of the policy between similar observations, each state needs to be explored separately)\n",
    "- large amount of memory required to store the Q-table, if implemented naively\n",
    "\n",
    "However, we can assume that only a subset of the observation space will be visited.  \n",
    "\n",
    "Task: Implement a representation of the Q-table that stores observations 'on-demand'.  \n",
    "Optional: Think of a way to store and reuse the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b35Zp1yNv8x0"
   },
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    \"\"\"\n",
    "    data structure to store the Q function for hashable state representations \n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions, initial_capacity=100):\n",
    "        self.capacity = initial_capacity\n",
    "        self.num_states = 0\n",
    "        self.state_index_map = {}\n",
    "        self.table:np.array = np.zeros([initial_capacity, n_actions])\n",
    "\n",
    "    def __contains__(self, state):\n",
    "        \"\"\" 'in' operator \"\"\"\n",
    "        return state in self.state_index_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_states\n",
    "\n",
    "    def __getitem__(self, state):\n",
    "        \"\"\" access state directly using [] notation \"\"\"\n",
    "        return self.table[self.state_index_map[state]]\n",
    "\n",
    "    def init_state(self, state):\n",
    "        if self.num_states == self.capacity: \n",
    "            # need to increase capacity\n",
    "            self.table = np.concatenate((self.table, np.zeros_like(self.table)))\n",
    "            self.capacity *= 2\n",
    "\n",
    "        self.state_index_map[state] = self.num_states\n",
    "        self.num_states += 1\n",
    "\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, \n",
    "            epsilon_start=0.5, epsilon_end=0.001,\n",
    "            epsilon_decay_length=10000, # in episodes\n",
    "            ):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.decay_step = (epsilon_start - epsilon_end) / epsilon_decay_length\n",
    "\n",
    "        self.env = env\n",
    "        self.Q = QTable(env.action_space.n)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" epsilon-greedy action selection \"\"\"\n",
    "        if not state in self.Q:\n",
    "            self.Q.init_state(state)\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        if random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state):\n",
    "        \"\"\" basic Q learning update \n",
    "            Q(s,a) <- Q(s,a) + alpha * [r + gamma * max(Q(s', .)) - Q(s,a)]\n",
    "        \"\"\"\n",
    "        if not next_state in self.Q:\n",
    "            self.Q.init_state(next_state)\n",
    "\n",
    "        td_error = reward + self.gamma * np.max(self.Q[next_state]) \\\n",
    "                    - self.Q[state][action]\n",
    "        self.Q[state][action] += self.alpha * td_error \n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.decay_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqbAleu6wXh2"
   },
   "source": [
    "## Training a Q-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mXs0iQ1A0yt7"
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(env, agent, n_episodes):\n",
    "    for e in range(n_episodes):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # convert to bytes so it can be used as dictionary key\n",
    "        # (np array is not hashable)\n",
    "        state = env.reset()\n",
    "        state = state.tobytes()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = next_state.tobytes()\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.update_Q(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "        # episode has finished\n",
    "        agent.decay_epsilon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aEJP8bpixMpC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "socket connection broken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-00c2e36d5f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'starting training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9d6ee64aee73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, agent, n_episodes)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/envs/mario_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mstate_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/marioai/gym-marioai/gym_marioai/protobuf_socket.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"socket connection broken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtotal_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DecodeVarint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: socket connection broken"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "episodes = 10000\n",
    "trials = 10\n",
    "alpha = 0.2\n",
    "gamma = 0.99\n",
    "\n",
    "# initialize agent and environment\n",
    "env = gym.make('Marioai-v0', render=True,\n",
    "                level_path=gym_marioai.levels.cliff_level,\n",
    "                rf_width=7, rf_height=5)\n",
    "agent = QAgent(env, alpha=alpha, gamma=gamma, epsilon_decay_length=episodes / 2)\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(f'starting training', trial)\n",
    "    train(env, agent, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bHte0ALwom6"
   },
   "source": [
    "## Plotting the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmlb_WctwxZn"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "marioai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
