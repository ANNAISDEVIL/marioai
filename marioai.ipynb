{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eH1YJeonXKg"
   },
   "source": [
    "# Training an Agent to play Super Mario\n",
    "In this exercise you are going to train a Q-Learning agent on the ```gym-marioai``` domain.  \n",
    "gym_marioai provides a python interface to interact with the MarioAI engine in a comfortable way. The engine itself is implemented in java, and the ```.jar``` of the engine needs to be started separately.  \n",
    "\n",
    "### Installation\n",
    "Requirements: Java 8 runtime environment, python 3.?  \n",
    "You will be provided with both the .jar and the gym-marioai python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TbJVNqroiOm"
   },
   "outputs": [],
   "source": [
    "# install the gym-environment\n",
    "# navigate to the source folder, then run:\n",
    "# pip install ./path/to/gym-marioai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH5X6wm7pQ7x"
   },
   "source": [
    "### Running the MarioAI server:\n",
    "navigate to the folder containing ```marioai-server.jar```, then run the following:  \n",
    "```java -jar ./marioai-server.jar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the server\n",
    "To run the server, run the following cell. It will launch the jar containing the Java engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen\n",
    "server_process = Popen(\n",
    "    ['java', '-jar', 'marioai-proto-interface/target/marioai-proto-interface-0.1-SNAPSHOT-jar-with-dependencies.jar'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the MarioAI server: (this also happens automatically by closing the render window)\n",
    "To kill the server process comment in the following cell and run it. Do not do this yet though ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_process.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoZ2eqZYpv2V"
   },
   "source": [
    "### python-client demo setup:\n",
    "make sure the demo is running..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_XSGD4ztdYy"
   },
   "source": [
    "## Representation of the Q-table\n",
    "You will experience some of the shortcomings of tabular reinforcement learning methods. With marioai, the observation space will be very large, resulting in\n",
    "- longer training duration (no interpolation of the policy between similar observations, each state needs to be explored separately)\n",
    "- large amount of memory required to store the Q-table, if implemented naively\n",
    "\n",
    "However, we can assume that only a subset of the observation space will be visited.  \n",
    "\n",
    "Task: Implement a representation of the Q-table that stores observations 'on-demand'.  \n",
    "Optional: Think of a way to store and reuse the trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqbAleu6wXh2"
   },
   "source": [
    "## Training a Q-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_marioai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    \"\"\"\n",
    "    data structure to store the Q function for hashable state representations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions, initial_capacity=100):\n",
    "        self.capacity = initial_capacity\n",
    "        self.num_states = 0\n",
    "        self.state_index_map = {}\n",
    "        self.table = np.zeros([initial_capacity, n_actions])\n",
    "\n",
    "    def __contains__(self, state):\n",
    "        \"\"\" 'in' operator \"\"\"\n",
    "        return state in self.state_index_map\n",
    "\n",
    "    #def __len__(self):\n",
    "    #    return self.num_states\n",
    "\n",
    "    def __getitem__(self, state):\n",
    "        \"\"\" access state directly using [] notation \"\"\"\n",
    "        if state not in self.state_index_map:\n",
    "            self.init_state(state)\n",
    "\n",
    "        return self.table[self.state_index_map[state]]\n",
    "\n",
    "    def init_state(self, state):\n",
    "        if self.num_states == self.capacity:\n",
    "            # need to increase capacity\n",
    "            self.table = np.concatenate(\n",
    "                (self.table, np.zeros_like(self.table)))\n",
    "            self.capacity *= 2\n",
    "        self.state_index_map[state] = self.num_states\n",
    "        self.num_states += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#   Training Parameters\n",
    "#####################################\n",
    "n_episodes = 5000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "lmbda = 0.75\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay_length = n_episodes / 2\n",
    "decay_step = (epsilon_end - epsilon_start) / epsilon_decay_length\n",
    "\n",
    "SAVE_FREQ = 100\n",
    "\n",
    "#####################################\n",
    "#   Environment/Reward Settings\n",
    "#####################################\n",
    "level = 'earlyCliffLevel'\n",
    "path = None\n",
    "\n",
    "if level == 'cliffLevel':\n",
    "    path = gym_marioai.levels.cliff_level\n",
    "if level == 'oneCliffLevel':\n",
    "    path = gym_marioai.levels.one_cliff_level\n",
    "if level == 'earlyCliffLevel':\n",
    "    path = gym_marioai.levels.early_cliff_level\n",
    "\n",
    "trace = 2\n",
    "rf_width = 20\n",
    "rf_height = 10\n",
    "prog = 1\n",
    "timestep = -1\n",
    "cliff = 1000\n",
    "win = -10\n",
    "dead = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.000196"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decay_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    training\n",
    "    \"\"\"\n",
    "    log_path = f'{level}_{rf_width}x{rf_height}_trace{trace}_prog{prog}_cliff{cliff}_win{win}_dead{dead}-0'\n",
    "    # logger = Logger(log_path)\n",
    "    # collect some training statistics\n",
    "    all_rewards = np.zeros([SAVE_FREQ])\n",
    "    all_wins = np.zeros([SAVE_FREQ])\n",
    "    all_steps = np.zeros([SAVE_FREQ])\n",
    "    all_gap_jumps = np.zeros([SAVE_FREQ])\n",
    "\n",
    "    ###################################\n",
    "    #       environment setup\n",
    "    ###################################\n",
    "    reward_settings = gym_marioai.RewardSettings(\n",
    "        progress=prog, timestep=timestep, cliff=cliff, win=win, dead=dead)\n",
    "    env = gym.make('Marioai-v0', render=False,\n",
    "                   level_path=path,\n",
    "                   reward_settings=reward_settings,\n",
    "                   compact_observation=True,\n",
    "                   trace_length=trace,\n",
    "                   rf_width=rf_width, rf_height=rf_height)\n",
    "\n",
    "    try:\n",
    "        ####################################\n",
    "        #       Q-learner setup\n",
    "        #####################################\n",
    "        Q = QTable(env.n_actions, 128)\n",
    "        etrace = {}\n",
    "\n",
    "        ####################################\n",
    "        #      Training Loop\n",
    "        ####################################\n",
    "        for e in range(n_episodes+1):\n",
    "            done = False\n",
    "            info = {}\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "            # exponential decay\n",
    "            #epsilon = (epsilon_end / epsilon_start) ** (e /\n",
    "            #                                            n_episodes) * epsilon_start\n",
    "            epsilon = max(epsilon_end, epsilon_start + e * decay_step)\n",
    "            \n",
    "            \n",
    "            state = env.reset()\n",
    "            #state = tuple([s.tobytes() for s in state])\n",
    "            # choose a' from a Policy derived from Q\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = int(np.argmax(Q[state]))  # greedy\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                #next_state = tuple([s.tobytes() for s in next_state])\n",
    "                total_reward += reward\n",
    "\n",
    "                # choose a' from a Policy derived from Q\n",
    "                best_next_action = int(np.argmax(Q[next_state]))  # greedy\n",
    "                if np.random.rand() < epsilon:\n",
    "                    next_action = env.action_space.sample()\n",
    "                else:\n",
    "                    next_action = best_next_action\n",
    "\n",
    "                # calculate the TD error\n",
    "                td_error = reward + gamma * \\\n",
    "                    Q[next_state][best_next_action] - Q[state][action]\n",
    "\n",
    "                # reset eligibility trace for (s,a) using replacing strategy\n",
    "                etrace[(state, action)] = 1\n",
    "\n",
    "                # perform Q update\n",
    "                if best_next_action == next_action:\n",
    "                    for (s, a), eligibility in etrace.items():\n",
    "                        Q[s][a] += alpha * eligibility * td_error\n",
    "                        etrace[(s, a)] *= gamma * lmbda\n",
    "                else:\n",
    "                    for (s, a), eligibility in etrace.items():\n",
    "                        Q[s][a] += alpha * eligibility * td_error\n",
    "                    etrace = {}\n",
    "\n",
    "                steps += 1\n",
    "                action = next_action\n",
    "                state = next_state\n",
    "\n",
    "            # episode finished\n",
    "            # logger.append(total_reward, info['steps'], info['win'])\n",
    "\n",
    "            all_rewards[e % SAVE_FREQ] = total_reward\n",
    "            all_wins[e % SAVE_FREQ] = 1 if info['win'] else 0\n",
    "            all_steps[e % SAVE_FREQ] = info['steps']\n",
    "            all_gap_jumps[e % SAVE_FREQ] = info['cliff_jumps']\n",
    "\n",
    "            if e % SAVE_FREQ == 0 and e > 0:\n",
    "                # logger.save()\n",
    "                # logger.save_model(Q)\n",
    "                print(f'Episode: {e} Eps: {epsilon:.3f}', f'Avg Reward: {all_rewards.mean():>4.2f}',\n",
    "                    f'Avg steps: {all_steps.mean():>4.2f}', f'Win% : {all_wins.mean():3.2f}',\n",
    "                    f'Cliff jumps: {all_gap_jumps.mean():.1f}', f'States seen: {Q.num_states}', end='\\r')\n",
    "        env.teardown()\n",
    "        return Q\n",
    "    except KeyboardInterrupt:\n",
    "        env.teardown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closing socket connection... Reward: 8405.42 Avg steps: 296.20 Win% : 0.00 Cliff jumps: 8.7 States seen: 713\n",
      "socket connection closed successfully.\n",
      "socket disconnected.\n"
     ]
    }
   ],
   "source": [
    "Q = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aEJP8bpixMpC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished episode. reward: 10158.00\t steps: 842.00\twin: False\t gap jumps: 11\n",
      "closing socket connection...\n",
      "socket connection closed successfully.\n",
      "socket disconnected.\n"
     ]
    }
   ],
   "source": [
    "reward_settings = gym_marioai.RewardSettings(progress=prog, timestep=timestep,\n",
    "                                             cliff=cliff, win=win, dead=dead)\n",
    "env = gym.make('Marioai-v0', render=True,\n",
    "               level_path=path,\n",
    "               reward_settings=reward_settings,\n",
    "               compact_observation=True,\n",
    "               trace_length=trace,\n",
    "               rf_width=rf_width, rf_height=rf_height)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        done = False\n",
    "        info = {}\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = int(np.argmax(Q[state]))  # greedy\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        print(f'finished episode. reward: {total_reward:4.2f}\\t steps: {steps:4.2f}\\t'\n",
    "            f'win: {info[\"win\"]}\\t gap jumps: {info[\"cliff_jumps\"]}')\n",
    "except KeyboardInterrupt:\n",
    "    env.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bHte0ALwom6"
   },
   "source": [
    "## Plotting the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmlb_WctwxZn"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_process.kill()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "marioai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
